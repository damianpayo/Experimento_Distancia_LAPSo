---
title: "Analisis de Potencia de Estudio de Percepción Auditiva de Distancia"
author: "Lucas Gonzalez - Damian Payo - Manuel Eguia"
date: "21/7/2023"
output:
  html_document:
    code_folding: "hide"
    theme: paper
    highlight: pygments
    toc: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readr)
library(dplyr)
library(here)
library(ggplot2)
library(plotrix)
library(segmented)
library(Routliers)
library(tidyverse)
library(ggpubr)
library(car)
library(data.table)
library(lme4)
library(nlme)
library(emmeans)
library(sjstats)
library(lmerTest)
library(MuMIn)
library(nortest)
```
## Resumen

Se expone el analisis de los resultados obtenidos en el experimento de PAD en los mese de Junio y Julio del 2023 en el aula 27 de la Universidad Nacional del Quilmes.

Se realizaron dos experimentos. En el experimento 1(a) participaron 23 sujetos (16 varones, 6 mujeres y 1 no binarie) con edades comprendidas entre los 19 y 60 años (promedio = 29, desviación estándar = 9.69). En el experimento 1(b) participaron 22 sujetos (14 varones y 8 mujeres) con edades comprendidas entre los 19 y 63 años (promedio = 34.36, desviación estándar = 12.45).

Experimento 1(a): Se expusieron 3 bloques de escucha con 3 condiciones (Wide, Low y High) con un filtro de corte de 1 kHz. Cada bloque está separado por un minuto de descanso.
Experimentos 1(b): Se expusieron 3 bloques de escucha con 3 condiciones (Wide, Low y High) con un filtro de corte de 4 kHz. Cada bloque está separado por un minuto de descanso.

## Carga de Datos

Los datos del experimento 1(a) se encuentran en la carpeta Data_1k. Consta de 69 csv correspondientes a cada condicion/bloque de cada sujeto.
- "nsub" es el ID otorgado al sujeto
- "distancia" corresponde a la distancia real del estímulo sonoro. Puede tomar 12 valores.
- "respuesta" corresponde a la respuesta del sujeto frente al estímulo
- "bloque" no es el dato real del orden de bloque. Mas adelante se lo corrige.

```{r Carga Datos, include=FALSE}
dir_1k <- "./Data_1k"
data_1k <- 
    list.files(path=dir_1k, pattern = "*.csv",full.names = TRUE) %>% 
    map_df(~fread(.) %>% mutate(trial_Order = row_number())) %>%
    mutate(Frec = "1k")

dir_4k <- "./Data_4k"
data_4k <- 
    list.files(path=dir_4k, pattern = "*.csv",full.names = TRUE) %>% 
    map_df(~fread(.)%>% mutate(trial_Order = row_number())) %>%
    mutate(Frec = "4k")

data <- rbind(data_1k, data_4k)
rm(dir_1k,dir_4k, data_1k, data_4k)
```
## Paso a log log y etiqueto

Aplicamos log(respuesta) y log(distancia) para poder hacer regresiones lineales entre las variales y obtener una pendiente que indique la compresión de distancia

log(respuesta) = log(distancia)**alpha + intercept

Donde "alpha" es la compresión
```{r, echo=FALSE, include=FALSE}
# Corregimos la columna de condicion con una etiqueta mas legible
# WW -> Sin filtro
# LL -> Filtro pasa-bajo
# HH -> Filtro pasa-alto
data$condicion[data$condicion==0] <- "WW"
data$condicion[data$condicion==1] <- "LL"
data$condicion[data$condicion==2] <- "HH"
data$distancia <- log(data$distancia) 
data$respuesta <- log(data$respuesta)
```
##  Detección y eliminación de Outliers por Respuesta

Aplicamos el criterio "Minimum Covariance Determinant" para detectar outliers multivariados.

En total de las 4968 observaciones se eliminaron 938 con este método.
```{r, echo=FALSE, warning=FALSE}
data_clean <- tibble()
Nsubs <- unique(data$nsub)
Conds <- unique(data$condicion)
for (N in Nsubs){
  for (C in Conds){
  data_N <- data %>% filter(nsub==N, condicion==C)
  outliers_N <- outliers_mcd(x=cbind(data_N$distancia,data_N$respuesta))
  outliers_list <- outliers_N$outliers_val

    # Eliminar outliers multivariados!
  outliers_index <-  which(data_N$distancia%in%outliers_list$X1 & 
                          data_N$respuesta%in%outliers_list$X2)

  if (length(outliers_index)!=0){
    data_N <- data_N[-outliers_index, ]
    }
  data_clean <- rbind(data_clean,data_N)
  }
}
rm(data_N,outliers_list,outliers_N,outliers_index,C,Conds,N,Nsubs)
```
## Regresiones Lineales

Se realizó una regresión lineal por cada condición por sujeto. Generamos un nuevo tibble que contiene un valor de pendiente (slope) por cada condición por sujeto (nsub).

Si slope vale 1 significa que la relación de compresión es 1 a 1, es decir que no hay compresión. Si es mayor a 1, significa que hay expansión. Si es menor a 1 significa que hay compresión de la distancia.

Además, se juntan todos los residuos (distancia a la regresión lineal) en un vector sresids. Por último se agrega el orden de los bloques correcto

```{r include=FALSE}
data_slopes <- tibble()
sresids <- vector()

Nsubs <- unique(data_clean$nsub)
Conds <- unique(data_clean$condicion)
for (N in Nsubs){
  for (C in Conds){
  data_N <- data_clean %>% filter(nsub==N, condicion==C)
  linear.model <- lm(respuesta~distancia,data_N)
  data_N$slope <- linear.model$coefficients[2]
  data_N$res <- as.vector(rstandard(linear.model))
  data_slopes <- rbind(data_slopes,data_N)
  sresids <- append(sresids, as.vector(rstandard(linear.model)))
  }
}
rm(data_N,C,Conds,N,Nsubs, linear.model)

data_clean <- data_slopes
data_slopes <- data_slopes %>%
  summarise(nsub, condicion, slope, Frec) %>%
  unique()

# Agrego el orden de los bloques
orden_bloques <- read.csv("./orden_bloques.csv")
data_slopes <- merge(x=orden_bloques,y=data_slopes,
                             by=c("nsub","condicion"))
rm(orden_bloques)
```
## Eliminamos al sujeto 3 por reportar problemas de audición

```{r, message=FALSE,error=FALSE,warning=FALSE,include=FALSE}
data_clean <- data_clean[!(data_clean$nsub==3), ]
data_slopes <- data_slopes[!(data_slopes$nsub==3), ]
```

### Observación de normalidad de los residuos observando

Graficamos un histograma y qqplot con los residuos de cada sujeto.

```{r TODOS LOS RESIDUOS (viejo)}
# Histogram & QQ Plot
hist(sresids, col='steelblue', main='Residues',xlab="Residues")
qqnorm(sresids, main='Normal')
qqline(sresids)
#qqPlot(sresids)
```
### Test Normalidad de los residuos Intrasujeto

Con el método Shapiro.Wilk se realiza un análisis de la normalidad de los residuos intrasujeto. Devuelve los nsubs que no permiten aprobar la h0 de que los residuos provienen de una distribución normal. Es decir, de todos los sujetos y condiciones evaluados, sólo 4 no presentan una distribución normal en los residuos (4-Wide, 12-Low, 16-Wide y 19-High)

```{r Residuos intra-sujeto, warning=FALSE, message=FALSE}
normality_test <- data_clean %>%
  group_by(nsub, condicion) %>%
  summarise(statistic = shapiro.test(res)$statistic,
            p.value = shapiro.test(res)$p.value)

normality_test%>%subset(p.value<0.05)
```
### Detección de Outliers por Pendientes y filtrado de Incompletos

Se aplica el criterio MAD (robust median absolute deviation) para eliminar outliers univariados que corresponden a las pendientes (slope).

Luego de quitan de la tabla los sujetos que no cuenten con una observación por cada condición.

```{r}
slope_1k_WW <- data_slopes %>% filter(Frec=="1k", condicion=="WW")
slope_1k_LL <- data_slopes %>% filter(Frec=="1k", condicion=="LL")
slope_1k_HH <- data_slopes %>% filter(Frec=="1k", condicion=="HH")

slope_4k_WW <- data_slopes %>% filter(Frec=="4k", condicion=="WW")
slope_4k_LL <- data_slopes %>% filter(Frec=="4k", condicion=="LL")
slope_4k_HH <- data_slopes %>% filter(Frec=="4k", condicion=="HH")

# 1k
outliers <-outliers_mad(slope_1k_WW$slope)
#outliers$limits[2]
slope_1k_WW<- slope_1k_WW[slope_1k_WW$slope<outliers$limits[2],]

outliers <-outliers_mad(slope_1k_LL$slope)
#outliers$limits[2]
slope_1k_LL<- slope_1k_LL[slope_1k_LL$slope<outliers$limits[2],]

outliers <-outliers_mad(slope_1k_HH$slope)
#outliers$limits[2]
slope_1k_HH<- slope_1k_HH[slope_1k_HH$slope<outliers$limits[2],]

# 4k
outliers <-outliers_mad(slope_4k_WW$slope)
#outliers$limits[2]
slope_4k_WW<- slope_4k_WW[slope_4k_WW$slope<outliers$limits[2],]

outliers <-outliers_mad(slope_4k_LL$slope)
#outliers$limits[2]
slope_4k_LL<- slope_4k_LL[slope_4k_LL$slope<outliers$limits[2],]

outliers <-outliers_mad(slope_4k_HH$slope)
#outliers$limits[2]
slope_4k_HH<- slope_4k_HH[slope_4k_HH$slope<outliers$limits[2],]

data_slopes_clean <- (rbind(slope_1k_HH,slope_1k_LL,slope_1k_WW,
                            slope_4k_HH,slope_4k_LL,slope_4k_WW))

count_conditions <- data_slopes_clean %>% 
  group_by(nsub) %>% 
  summarise(n = n())
drops <- count_conditions %>% subset(n!=3)
data_slopes_clean <- data_slopes_clean[!(data_slopes_clean$nsub%in%
                                                 drops$nsub), ]

rm(slope_1k_HH, slope_1k_LL,slope_1k_WW,
   slope_4k_HH,slope_4k_LL,slope_4k_WW)
rm(outliers, count_conditions)
```

### Test Normalidad Slopes

Aplicamos Shapiro-Wilk para verificar si los datos pueden provenir de una distribución normal.
En tal caso, los datos que no provienen de una distribución normal son los correspondientes a 
la condicion "Low", tanto para el filtro de 1kHz como para el de 4kHz.

```{r, message=FALSE}
normality_slopes <- data_slopes_clean %>%
  group_by(Frec, condicion) %>%
  summarise(statistic = shapiro.test(slope)$statistic,
            p.value = shapiro.test(slope)$p.value, 
            normality=(p.value>0.05))

normality_slopes
```

## Boxplot  con las Pendientes

Se grafica en un boxplot el total de las pendientes por condicion para 1kHz y para 4kHz, 
haciendo mención al grado de significancia de los datos.
```{r, message=FALSE, warning=FALSE}
cols <- c("#455A64", "#90A4AE", "#CFD8DC" )

plt_1k_slope <- ggplot(data_slopes_clean %>% 
                         filter(Frec=="1k"), 
                       aes(x=condicion, y=slope, fill=condicion)) + 
  stat_boxplot(geom = "errorbar",
               width = 0.25) + 
  geom_boxplot(alpha=0.8, outlier.colour=NA, width = 0.4)+
  scale_x_discrete(limits = c('WW', 'LL', 'HH'), 
                   labels = c('Wide Band', 'Low Pass', 'High Pass')) +
  scale_fill_manual(values = cols, guide='none') +
  scale_y_continuous(limits=c(0,2)) +
  labs(title="Cut Frecuency = 1 kHz", y='Slopes')+
  annotate("text", x = 1.5, y = 1.6,  label = "***", size = 4) +
  annotate("segment", x = 0.7, xend = 2.2, y = 1.55, 
           yend = 1.55, colour = "black", size=.3, alpha=1,) +
  annotate("text", x = 2.5, y = 1.8,  label = "***", size = 4) +
  annotate("segment", x = 1.75, xend = 3.2, y = 1.75, 
           yend = 1.75, colour = "black", size=.3, alpha=1,) +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_text(size = 9),
        plot.title = element_text(size = 10, hjust = 0.5))

plt_4k_slope <- ggplot(data_slopes_clean %>% 
                         filter(Frec=="4k" & slope < 1.6), 
                       aes(x=condicion, y=slope, fill=condicion)) + 
  stat_boxplot(geom = "errorbar",
               width = 0.25) + 
  geom_boxplot(alpha = 0.8, outlier.colour=NA, width = 0.4)+
  scale_x_discrete(limits = c('WW', 'LL', 'HH'),
                   labels = c('Wide Band', 'Low Pass', 'High Pass')) +
  scale_fill_manual(values = cols, guide='none') +
  scale_y_continuous(limits=c(0,2)) +
  labs(title="Cut Frecuency = 4 kHz")+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.title = element_text(size = 10, hjust = 0.5))

ggarrange(plt_1k_slope,plt_4k_slope, common.legend = TRUE)
```
```{r eval=FALSE, include=FALSE}
ggplot(data_clean %>% filter(Frec == '1k'), aes(x=distancia, y=respuesta, color=condicion)) +
  geom_point(alpha = 1/2, size = 1) +
  geom_smooth(method = lm, se = FALSE, size=0.5) +
  facet_wrap(~nsub) +
  theme_minimal()
# ggplot(data_clean %>% filter(Frec == '1k', condicion == 'WW', nsub == 20), aes(x=distancia, y=respuesta)) +
#   geom_point(alpha = 1/2, size = 1) +
#   geom_smooth(method = lm, se = FALSE, size=0.5) +
#   facet_wrap(~nsub) +
#   theme_minimal()
```

## T-test

Se exponen los resultados en donde sólo se presenta una diferencia significativa 
entre Low-Wde y Low-High para 1kHz (esto se muestra en el boxplot anterior).
```{r}
slope_1k_WW <- data_slopes_clean %>% 
  filter(Frec=="1k", condicion=="WW")
slope_1k_LL <- data_slopes_clean %>% 
  filter(Frec=="1k", condicion=="LL")
slope_1k_HH <- data_slopes_clean %>% 
  filter(Frec=="1k", condicion=="HH")
slope_4k_WW <- data_slopes_clean %>% 
  filter(Frec=="4k", condicion=="WW")
slope_4k_LL <- data_slopes_clean %>% 
  filter(Frec=="4k", condicion=="LL")
slope_4k_HH <- data_slopes_clean %>% 
  filter(Frec=="4k", condicion=="HH")

WW_LL_1k <- t.test(x=slope_1k_WW$slope, slope_1k_LL$slope)
WW_HH_1k <- t.test(x=slope_1k_WW$slope, slope_1k_HH$slope)
LL_HH_1k <- t.test(x=slope_1k_LL$slope, slope_1k_HH$slope)

WW_LL_4k <- t.test(x=slope_4k_WW$slope, slope_4k_LL$slope)
WW_HH_4k <- t.test(x=slope_4k_WW$slope, slope_4k_HH$slope)
LL_HH_4k <- t.test(x=slope_4k_LL$slope, slope_4k_HH$slope)

T_test_condicion <- list(WW_LL_1k = WW_LL_1k$p.value, 
                    WW_HH_1k = WW_HH_1k$p.value, 
                    LL_HH_1k = LL_HH_1k$p.value,
                    WW_LL_4k = WW_LL_4k$p.value,
                    WW_HH_4k = WW_HH_4k$p.value,
                    LL_HH_4k = LL_HH_4k$p.value)

# Convertimos la lista en un tibble
T_test_condicion <- tibble(Test = names(T_test_condicion), P_Value = unlist(T_test_condicion)) %>% mutate(significance=P_Value<0.05)

rm(WW_HH_1k,LL_HH_1k,WW_LL_1k,WW_LL_4k,WW_HH_4k,LL_HH_4k)
# Mostramos el tibble con los resultados
T_test_condicion
```
## AnOVa a partir del modelo lineal

```{r 1k_1}
slopes_1k_clean <- data_slopes_clean%>%filter(Frec=="1k")

lm.1k <-lm(slope~condicion,
               data=slopes_1k_clean)
# summary(lm.1k)
# Este anova no explica cual es el más extremo. Si podemos rechazar la h0 de que todas las medias son iguales
anova(lm.1k)

pairwise.t.test(slopes_1k_clean$slope,
                slopes_1k_clean$condicion,
                p.adjust.method = "bonferroni")
```

```{r 4k_1}
slopes_4k_clean <- data_slopes_clean%>%filter(Frec=="4k")

lm_4k <-lm(slope ~ condicion,
                data=slopes_4k_clean)
#summary(lm_4k)
# Este anova no explica cual es el más extremo. Si podemos rechazar la h0 de que todas las medias son iguales
anova(lm_4k)

pairwise.t.test(slopes_4k_clean$slope,
                slopes_4k_clean$condicion, 
                p.adjust.method = "bonferroni")
```
## Modelo Lineal con Efectos Mixtos

Referencia de linear mixed model effect: 
https://www.youtube.com/watch?v=AWInLxpiZuA

### LMER en 1k
```{r 1k_2}
lmer.1k <- lmer(slope ~ condicion + (1|nsub),
    data = slopes_1k_clean)

anova(lmer.1k)

##partial eta sq
effectsize::eta_squared(lmer.1k, partial = TRUE)
#Rsq
r.squaredGLMM(lmer.1k)  

emmeans(lmer.1k, 
        list(pairwise ~ condicion),
        adjust="bonferroni")

qqnorm(resid(lmer.1k))
```
### LMER en 4k

```{r 4k_2}
lmer.4k <- lmer(slope ~ condicion + (1|nsub),
    data = slopes_4k_clean)

anova(lmer.4k)

##partial eta sq
effectsize::eta_squared(lmer.4k, partial = TRUE)
#Rsq
r.squaredGLMM(lmer.4k)  

emmeans(lmer.4k, 
        list(pairwise ~ condicion),
        adjust="bonferroni")

qqnorm(resid(lmer.4k))
```


## EXTRA: Grafico por orden de bloques

```{r COMPARO}

# ggplot(data=slopes_corregida%>%filter(Frec=="1k"), aes(x=as.character(orden), y=slope,color=condicion))+
#   geom_boxplot()+
#   geom_point(position = "jitter")+
#   xlab("Bloque")+
#   ylab("Pendiente")+
#   theme_minimal()

plt_1k_per_block <- ggplot(slopes_1k_clean,
                           aes(x=as.character(orden),
                               y=slope,color=condicion)) + 
  geom_boxplot()+
  #geom_point(position = "jitter")+
  xlab("Bloque")+
  ylab("Pendiente")+
  labs(title="Pendientes por bloque 1k")+
  theme_minimal()

plt_4k_per_block <- ggplot(slopes_4k_clean,
                           aes(x=as.character(orden),
                               y=slope,color=condicion)) + 
  geom_boxplot()+
  #geom_point(position = "jitter")+
  xlab("Bloque")+
  ylab("Pendiente")+
  labs(title="Pendientes por bloque  4k")+
  theme_minimal()

ggarrange(plt_1k_per_block,plt_4k_per_block, common.legend = TRUE)
```
